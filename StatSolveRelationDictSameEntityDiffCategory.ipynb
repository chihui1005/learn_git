{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stat_different_amount =  384179\n"
     ]
    }
   ],
   "source": [
    "#  统计所有关系字典文件中实体的自对齐问题\n",
    "\n",
    "def readLinesFile(read_file_path):\n",
    "    with open(read_file_path,\"r\",encoding=\"utf-8\") as fp:\n",
    "        lines=fp.readlines()\n",
    "    return lines\n",
    "\n",
    "def writeJsonFile(dictLineJson,write_json_file_path):\n",
    "    with open(write_json_file_path,\"a\",encoding=\"utf-8\") as fp_w_json:\n",
    "        json.dump(dictLineJson,fp_w_json)  #write file\n",
    "        fp_w_json.write(\"\\n\")\n",
    "\n",
    "def statRelDictsSameEntityDifferentCategoryAmount(read_entity_relation_dict_dir ,write_stat_path ):\n",
    "    # header = [\"entity\",\"\"]\n",
    "    stat_dict = {}\n",
    "    rel_dict_file_list = os.listdir(read_entity_relation_dict_dir)\n",
    "    stat_different_amount = 0\n",
    "\n",
    "    for rel_dict_file_name in rel_dict_file_list:\n",
    "        if rel_dict_file_name == \".DS_Store\":\n",
    "            continue\n",
    "        rel_dict_path = os.path.join(read_entity_relation_dict_dir,rel_dict_file_name)\n",
    "        lines = readLinesFile(rel_dict_path)\n",
    "        c1 = rel_dict_file_name.split(\"_\")[0]\n",
    "        c2 = rel_dict_file_name.split(\"_\")[1]\n",
    "        two_key = rel_dict_file_name[:-5]\n",
    "        for line in lines:\n",
    "            line = json.loads(line)\n",
    "            e1, e2 =line[\"e1\"], line[\"e2\"]\n",
    "            source = line[\"sources\"]\n",
    "            # if c1 == c2 or e1 == e2:\n",
    "            #     continue\n",
    "            if e1 not in stat_dict:\n",
    "                # stat_dict[e1] = {two_key: [c1]}\n",
    "                stat_dict[e1] ={two_key :{c1:source}}\n",
    "                stat_different_amount+=1\n",
    "            else:\n",
    "                if two_key not in stat_dict[e1]:\n",
    "                    # stat_dict[e1][two_key] = [c1]\n",
    "                    stat_dict[e1][two_key] = {c1:source}\n",
    "                    stat_different_amount += 1\n",
    "\n",
    "                if c1 not in stat_dict[e1][two_key]: # normal unexist!!1\n",
    "                    # stat_dict[e1][two_key].append(c1)\n",
    "                    stat_dict[e1][two_key][c1] =source\n",
    "                    stat_different_amount += 1\n",
    "\n",
    "            if e2 not in stat_dict:\n",
    "                # stat_dict[e2] ={two_key :[c2]}\n",
    "                stat_dict[e2] = {two_key: {c2:source}}\n",
    "                stat_different_amount+=1\n",
    "\n",
    "            else:\n",
    "                if two_key not in stat_dict[e2]:\n",
    "                    # stat_dict[e2][two_key] = [c2]\n",
    "                    stat_dict[e2][two_key] = {c2:source}\n",
    "                    stat_different_amount += 1\n",
    "\n",
    "                if c2 not in stat_dict[e2][two_key]:\n",
    "                    # stat_dict[e2][two_key].append(c2)\n",
    "                    stat_dict[e2][two_key][c2] =source\n",
    "                    stat_different_amount += 1\n",
    "\n",
    "            # stat_dict[e1][rel_dict_file_name[:-5]] = c1\n",
    "            # stat_dict[e2][rel_dict_file_name[:-5]] = c2\n",
    "    for key,value in stat_dict.items():\n",
    "        # value_v_list = list(value.values())\n",
    "        value_v_list = []\n",
    "        temp_set= set()\n",
    "        for value_k,value_v in value.items():\n",
    "            # value_v_list = list(value_v.keys())\n",
    "            for value_v_k, value_v_v in value_v.items():\n",
    "                temp_set.add(value_v_k)\n",
    "        value_v_list = list(temp_set)\n",
    "        value_v_list_len  = len(value_v_list)\n",
    "        if value_v_list_len == 1:\n",
    "            continue\n",
    "        for i in range(1,value_v_list_len):\n",
    "            if value_v_list[i-1] == value_v_list[1]:\n",
    "                continue\n",
    "                \n",
    "            else:\n",
    "                # print(\"write data...\")\n",
    "                writeJsonFile({key:value}, write_stat_path)\n",
    "                break\n",
    "\n",
    "\n",
    "    print(\"stat_different_amount = \",stat_different_amount)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #todo: note: execute below function, the order is very important!!!\n",
    "    #todo: statistics not align entity!!!, this is the first statistics!\n",
    "    read_entity_relation_dict_dir = \"./quchong_relation_dict/\"\n",
    "    write_stat_path =\"./rel_dicts_same_entity_category_different_1.json\"\n",
    "    statRelDictsSameEntityDifferentCategoryAmount(read_entity_relation_dict_dir, write_stat_path)   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start run time : time.struct_time(tm_year=2019, tm_mon=6, tm_mday=28, tm_hour=17, tm_min=41, tm_sec=18, tm_wday=4, tm_yday=179, tm_isdst=0)\n",
      "stat_different_amount =  341008\n",
      "End run time : time.struct_time(tm_year=2019, tm_mon=6, tm_mday=28, tm_hour=17, tm_min=42, tm_sec=7, tm_wday=4, tm_yday=179, tm_isdst=0)\n"
     ]
    }
   ],
   "source": [
    "#  统计 并解决各个关系字典的实体自对齐问题\n",
    "import json\n",
    "import csv\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "\n",
    "def readJsonFile(read_json_file_path):\n",
    "    with open(read_json_file_path,\"r\",encoding=\"utf-8\") as fp_r_json:\n",
    "        json_data =json.load(fp_r_json)\n",
    "    return json_data\n",
    "def writeJsonFile(dictLineJson,write_json_file_path):\n",
    "    with open(write_json_file_path,\"a\",encoding=\"utf-8\") as fp_w_json:\n",
    "        json.dump(dictLineJson,fp_w_json)  #write file\n",
    "        fp_w_json.write(\"\\n\")\n",
    "def readLinesFile(read_file_path):\n",
    "    with open(read_file_path,\"r\",encoding=\"utf-8\") as fp:\n",
    "        lines=fp.readlines()\n",
    "    return lines\n",
    "\n",
    "def writeCSV(header_content,write_file_path):\n",
    "    with open(write_file_path,\"w\",encoding=\"utf-8\")as fp_w_csv:\n",
    "        write_csv = csv.writer(fp_w_csv,dialect=\"excel\")\n",
    "        write_csv.writerows(header_content)\n",
    "\n",
    "\n",
    "def statRelDictsSameEntityDifferentCategoryAmount(read_entity_relation_dict_dir ,write_stat_path ):\n",
    "    # header = [\"entity\",\"\"]\n",
    "    stat_dict = {}\n",
    "    rel_dict_file_list = os.listdir(read_entity_relation_dict_dir)\n",
    "    stat_different_amount = 0\n",
    "\n",
    "    for rel_dict_file_name in rel_dict_file_list:\n",
    "        if rel_dict_file_name == \".DS_Store\":\n",
    "            continue\n",
    "        rel_dict_path = os.path.join(read_entity_relation_dict_dir,rel_dict_file_name)\n",
    "        lines = readLinesFile(rel_dict_path)\n",
    "        c1 = rel_dict_file_name.split(\"_\")[0]\n",
    "        c2 = rel_dict_file_name.split(\"_\")[1]\n",
    "        two_key = rel_dict_file_name[:-5]\n",
    "        for line in lines:\n",
    "            line = json.loads(line)\n",
    "            e1, e2 =line[\"e1\"], line[\"e2\"]\n",
    "            source = line[\"sources\"]\n",
    "            # if c1 == c2 or e1 == e2:\n",
    "            #     continue\n",
    "            if e1 not in stat_dict:\n",
    "                # stat_dict[e1] = {two_key: [c1]}\n",
    "                stat_dict[e1] ={two_key :{c1:source}}\n",
    "                stat_different_amount+=1\n",
    "            else:\n",
    "                if two_key not in stat_dict[e1]:\n",
    "                    # stat_dict[e1][two_key] = [c1]\n",
    "                    stat_dict[e1][two_key] = {c1:source}\n",
    "                    stat_different_amount += 1\n",
    "\n",
    "                if c1 not in stat_dict[e1][two_key]: # normal unexist!!1\n",
    "                    # stat_dict[e1][two_key].append(c1)\n",
    "                    stat_dict[e1][two_key][c1] =source\n",
    "                    stat_different_amount += 1\n",
    "\n",
    "            if e2 not in stat_dict:\n",
    "                # stat_dict[e2] ={two_key :[c2]}\n",
    "                stat_dict[e2] = {two_key: {c2:source}}\n",
    "                stat_different_amount+=1\n",
    "\n",
    "            else:\n",
    "                if two_key not in stat_dict[e2]:\n",
    "                    # stat_dict[e2][two_key] = [c2]\n",
    "                    stat_dict[e2][two_key] = {c2:source}\n",
    "                    stat_different_amount += 1\n",
    "\n",
    "                if c2 not in stat_dict[e2][two_key]:\n",
    "                    # stat_dict[e2][two_key].append(c2)\n",
    "                    stat_dict[e2][two_key][c2] =source\n",
    "                    stat_different_amount += 1\n",
    "\n",
    "            # stat_dict[e1][rel_dict_file_name[:-5]] = c1\n",
    "            # stat_dict[e2][rel_dict_file_name[:-5]] = c2\n",
    "    for key,value in stat_dict.items():\n",
    "        # value_v_list = list(value.values())\n",
    "        value_v_list = []\n",
    "        temp_set= set()\n",
    "        for value_k,value_v in value.items():\n",
    "            # value_v_list = list(value_v.keys())\n",
    "            for value_v_k, value_v_v in value_v.items():\n",
    "                temp_set.add(value_v_k)\n",
    "        value_v_list = list(temp_set)\n",
    "        value_v_list_len  = len(value_v_list)\n",
    "        if value_v_list_len == 1:\n",
    "            continue\n",
    "#        for i in range(1,value_v_list_len):\n",
    "#            if value_v_list[i-1] == value_v_list[1]:\n",
    "#                continue\n",
    "        else:\n",
    "            # print(\"write data...\")\n",
    "            writeJsonFile({key:value}, write_stat_path)\n",
    "#            break\n",
    "\n",
    "\n",
    "    print(\"stat_different_amount = \",stat_different_amount)\n",
    "\n",
    "# according to the rule of lijun, each relation dict files are aligned!!!\n",
    "def relationDictFilesAlign(read_all_relation_map_path,read_stat_diff_category_file_path,read_entity_relation_dict_dir,write_old_rel_dict_dir):\n",
    "    start_time =time.localtime(time.time())\n",
    "    print(\"star time: \",start_time)\n",
    "    align_rule = {\"protein_gene\":\"gene\",\"drugs_gene\" : \"gene\",\"disease_symptom\" : \"disease\",\"compound_drugs\":\"compound\",\"anatomy\":\"anatomy\"}\n",
    "    all_relation_map = readJsonFile(read_all_relation_map_path)\n",
    "    stat_lines = readLinesFile(read_stat_diff_category_file_path)\n",
    "    modify_corresponding_to_files_and_entity = {} #save should align file of relation dict and entity corresponding to category\n",
    "\n",
    "    print(\"start obtain statRelDictsSameEntityDifferentCategoryAmount and aligned entities that change as needed by the file\")\n",
    "    for line in stat_lines:\n",
    "        line = json.loads(line)\n",
    "        temp_set = set() #\n",
    "        temp_list = []\n",
    "        for key, value in line.items():  # only key = entity ,value\n",
    "            #todo: add source !!!\n",
    "            for key_k,value_v in value.items():\n",
    "                for value_v_k,value_v_v in value_v.items():\n",
    "                    temp_set.add(value_v_k) # obtain all different category!!\n",
    "            if \"anatomy\" in temp_set:\n",
    "                for k, v in value.items():\n",
    "                    for v_k,v_v in v.items():\n",
    "                        # for v_v_k ,v_v_v in v_v.items():\n",
    "                        if v_k != \"anatomy\":\n",
    "                            if k + \".dict\" not in modify_corresponding_to_files_and_entity:\n",
    "                                modify_corresponding_to_files_and_entity[k + \".dict\"] = {key: \"anatomy\"}\n",
    "                            else:\n",
    "                                modify_corresponding_to_files_and_entity[k + \".dict\"][key] = \"anatomy\"\n",
    "\n",
    "\n",
    "            elif len(temp_set) > 2 : # Since there is no rule, it will not be processed yet\n",
    "                continue\n",
    "            else:\n",
    "                c1 = temp_set.pop()\n",
    "                c2 = temp_set.pop()\n",
    "                if c1 + \"_\" + c2 in align_rule :\n",
    "                    align_category = align_rule[c1 + \"_\" + c2]\n",
    "                    for k, v in value.items():\n",
    "                        for v_k,v_v in v.items():\n",
    "                            if v_k != align_category:\n",
    "                                # temp_list.append(k + \".dict\")\n",
    "                                if k + \".dict\" not in modify_corresponding_to_files_and_entity:\n",
    "                                    modify_corresponding_to_files_and_entity[k + \".dict\"] = {key: align_category}\n",
    "                                else:\n",
    "                                    modify_corresponding_to_files_and_entity[k + \".dict\"][key] = align_category\n",
    "\n",
    "                elif c2 + \"_\" + c1 in align_rule :\n",
    "                    align_category = align_rule[c2 + \"_\" + c1]\n",
    "                    for k, v in value.items():\n",
    "                        for v_k, v_v in v.items():\n",
    "                            if v_k != align_category:\n",
    "                                if k + \".dict\" not in modify_corresponding_to_files_and_entity:\n",
    "                                    modify_corresponding_to_files_and_entity[k + \".dict\"] = {key: align_category}\n",
    "                                else:\n",
    "                                    modify_corresponding_to_files_and_entity[k + \".dict\"][key] = align_category\n",
    "\n",
    "    print(\"finish obtain !!!\")\n",
    "#    print(modify_corresponding_to_files_and_entity)\n",
    "    print(\"start deal with align relation dicts...\")\n",
    "    for key_rel_dict_file, value_entity_category in modify_corresponding_to_files_and_entity.items():\n",
    "        print(\"start deal with {} file\".format(key_rel_dict_file))\n",
    "        read_rel_dict_path = os.path.join(read_entity_relation_dict_dir,key_rel_dict_file)\n",
    "        print(read_rel_dict_path)\n",
    "        rel_dict_lines = readLinesFile(read_rel_dict_path)\n",
    "        os.remove(read_rel_dict_path) #delete the file\n",
    "        c1,c2 = key_rel_dict_file.split(\"_\")[:-1]\n",
    "        for line in rel_dict_lines:\n",
    "            line =json.loads(line)\n",
    "            e1 = line[\"e1\"]\n",
    "            e2 = line[\"e2\"]\n",
    "            if e1 not in value_entity_category and e2 not in value_entity_category:\n",
    "                writeJsonFile(line,os.path.join(write_old_rel_dict_dir,key_rel_dict_file)) #read_rel_dict_path == write_rel_dict_path\n",
    "                continue\n",
    "            if (e1 in value_entity_category and c1 != value_entity_category[e1]) and (e2 in value_entity_category and c2 != value_entity_category[e2]): # two entity need to align\n",
    "                align_c1 = value_entity_category[e1]\n",
    "                align_c2 = value_entity_category[e2]\n",
    "                if align_c1+\"_\"+align_c2 in all_relation_map:\n",
    "                    writeJsonFile(line, os.path.join(write_old_rel_dict_dir,all_relation_map[align_c1+\"_\"+align_c2]))  # read_rel_dict_path == write_rel_dict_path\n",
    "#                elif align_c2+\"_\"+align_c1 in all_relation_map:\n",
    "#                    writeJsonFile(line, os.path.join(write_old_rel_dict_dir,all_relation_map[align_c2+\"_\"+align_c1]))  # read_rel_dict_path == write_rel_dict_path\n",
    "            elif (e1 in value_entity_category and c1 != value_entity_category[e1]) and e2 not in value_entity_category:\n",
    "                align_c1 = value_entity_category[e1]\n",
    "                if align_c1 +\"_\"+ c2 in all_relation_map:\n",
    "                    writeJsonFile(line, os.path.join(write_old_rel_dict_dir,all_relation_map[align_c1+\"_\"+ c2]))\n",
    "#                elif c2+\"_\"+align_c1 in all_relation_map:\n",
    "#                    writeJsonFile(line, os.path.join(write_old_rel_dict_dir,all_relation_map[c2+\"_\"+align_c1]))  # read_rel_dict_path == write_rel_dict_path\n",
    "            elif (e2 in value_entity_category and c2 != value_entity_category[e2]) and e1 not in value_entity_category:\n",
    "                align_c2 = value_entity_category[e2]\n",
    "                if c1+\"_\"+align_c2 in all_relation_map:\n",
    "                    writeJsonFile(line, os.path.join(write_old_rel_dict_dir,all_relation_map[c1+\"_\"+align_c2]))\n",
    "#                elif align_c2+\"_\"+c1 in all_relation_map:\n",
    "#                    writeJsonFile(line, os.path.join(write_old_rel_dict_dir,all_relation_map[align_c2+\"_\"+c1]))  # read_rel_dict_path == write_rel_dict_path\n",
    "        print(\"finish  {} file deal with\".format(key_rel_dict_file))\n",
    "    end_time = time.localtime(time.time())\n",
    "    print(\"end time \",end_time)\n",
    "\n",
    "\n",
    "def judgeDrugsWhetherInDrugBank(read_drugbank_file_path,read_stat_not_align_file_path,write_stat_dir):\n",
    "    drugbank_lines = readLinesFile(read_drugbank_file_path)\n",
    "    drugbank_set = set()\n",
    "    for line in drugbank_lines:\n",
    "        line =json.loads(line)\n",
    "        drugbank_set.add(line[\"name\"])\n",
    "        drugbank_set = drugbank_set | set(line[\"synonym_list\"])\n",
    "    stat_not_align_entity_lines = readLinesFile(read_stat_not_align_file_path)\n",
    "    # num = 1\n",
    "    for stat_line in stat_not_align_entity_lines:\n",
    "        stat_line = json.loads(stat_line)\n",
    "        stat_temp_category = set()\n",
    "        # num+=1\n",
    "        # if num ==561:\n",
    "        #     print(stat_line)\n",
    "        for key, value in stat_line.items(): # only one group key, value\n",
    "            for value_k,value_v in value.items():\n",
    "                for value_v_k, value_v_v in value_v.items():\n",
    "                    stat_temp_category.add(value_v_k)\n",
    "            if \"drugs\" in stat_temp_category :\n",
    "                if key in drugbank_set:\n",
    "                    writeJsonFile(stat_line,os.path.join(write_stat_dir,\"align_drug.json\"))\n",
    "                    break\n",
    "                else:\n",
    "                    writeJsonFile(stat_line,os.path.join(write_stat_dir,\"drugs_not_align_drugsbank.json\"))\n",
    "            else:\n",
    "                writeJsonFile(stat_line, os.path.join(write_stat_dir, \"manual_verification.json\"))\n",
    "\n",
    "#todo: It is convenient  for lijun to mark by csv format\n",
    "def statFormatCSV(read_stat_dir,write_stat_csv_format_dir):\n",
    "    stat_file_list = os.listdir(read_stat_dir)\n",
    "    for stat_file_name in stat_file_list:\n",
    "        stat_file_path = os.path.join(read_stat_dir,stat_file_name)\n",
    "        stat_lines = readLinesFile(stat_file_path)\n",
    "        header_content = []\n",
    "        # if stat_file_name == \"manual_verification.json\": #debug error by lijun\n",
    "        #     print(stat_file_name)\n",
    "        # num = 1\n",
    "        for line in stat_lines:\n",
    "        #    num+=1\n",
    "            line = json.loads(line)\n",
    "            temp_ls = []\n",
    "            for key,value in line.items():\n",
    "                # if key == \"carbonyl sulfide\" or num == 536: #debug: error by lijun\n",
    "                #     print(key)\n",
    "                temp_ls.append(key)\n",
    "                for value_k,value_v in value.items():\n",
    "                    for value_v_k,value_v_v in value_v.items():\n",
    "                        temp_ls.append(value_v_k)\n",
    "            header_content.append(temp_ls)\n",
    "\n",
    "        write_csv_stat_file_path =os.path.join(write_stat_csv_format_dir,stat_file_name[:-4]+\"csv\")\n",
    "        writeCSV(header_content, write_csv_stat_file_path)\n",
    "\n",
    "\n",
    "# obtain need to align entity, which obey the rule by lijun!\n",
    "def obtainAlignEntity(read_file_path):\n",
    "    align_entity_set = set()\n",
    "    lines = readLinesFile(read_file_path)\n",
    "    for line in lines:\n",
    "        line = json.loads(line)\n",
    "        for key , value in line.items():\n",
    "            align_entity_set.add(key)\n",
    "    return align_entity_set\n",
    "\n",
    "# obtain align entity by lijun verification!\n",
    "def readManualVertificationAlignEntityCSV(read_csv_file_path):\n",
    "    manual_vertification_align_entity_dict = {}\n",
    "    with open(read_csv_file_path,\"r\",encoding=\"utf-8\") as fp_r_csv:\n",
    "        read_csv = csv.reader(fp_r_csv)\n",
    "        for line in read_csv:\n",
    "            manual_vertification_align_entity_dict[line[0]] = line[1]\n",
    "    return manual_vertification_align_entity_dict\n",
    "\n",
    "\n",
    "# acording to the rule of lijun, each relation dict files are aligned!!!, new rule of drugs and manual vertification entity!\n",
    "def relationDictFilesAlignImprove(read_all_relation_map_path,read_stat_2_diff_category_file_path,read_align_drugs_entity_path,read_not_align_drugs_entity_path,read_manual_vertify_align_entity_path,read_entity_relation_dict_dir,write_old_rel_dict_dir):\n",
    "    start_time =time.localtime(time.time())\n",
    "    print(\"star time: \",start_time)\n",
    "    all_relation_map = readJsonFile(read_all_relation_map_path)\n",
    "    stat_lines = readLinesFile(read_stat_2_diff_category_file_path) #statistics need to algin entity\n",
    "\n",
    "    #todo: the result of statistics different category of entity is unique by statRelDictsSameEntityDifferentCategoryAmount function.\n",
    "    align_drugs_entity_set = obtainAlignEntity(read_align_drugs_entity_path)  # these entity should align to cagegory of drugs!\n",
    "    not_align_drugs_entity_set = obtainAlignEntity(read_not_align_drugs_entity_path) # entity contain category of drugs, but the entity not contain in drugbank!!!, so remove entpair\n",
    "    manual_vertification_align_entity_dict =readManualVertificationAlignEntityCSV(read_manual_vertify_align_entity_path) # these entity are vertificated by lijun.\n",
    "    modify_corresponding_to_files_and_entity = {} #{\"rel_dict_file_1\":{\"e1\":\"c1\",\"e2\":\"c2\",...},...} should save align file of relation dict and entity corresponding to category\n",
    "\n",
    "    print(\"start obtain statRelDictsSameEntityDifferentCategoryAmount and aligned entities that change as needed by the file\")\n",
    "    for line in stat_lines:\n",
    "        line = json.loads(line)\n",
    "        temp_set = set() #\n",
    "        for key, value in line.items():  # only key = entity ,value\n",
    "            # deal with align drugs entity!\n",
    "            if key in align_drugs_entity_set:\n",
    "                for value_k,value_v in value.items():\n",
    "                    if value_k+\".dict\" not in modify_corresponding_to_files_and_entity:\n",
    "                        modify_corresponding_to_files_and_entity[value_k+\".dict\"] ={key:\"drugs\"}\n",
    "                        continue\n",
    "                    else:\n",
    "                        if key not in modify_corresponding_to_files_and_entity[value_k+\".dict\"]:\n",
    "                            modify_corresponding_to_files_and_entity[value_k + \".dict\"][key] = \"drugs\"\n",
    "                            continue\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "            # record delete not align drugs entity!!!\n",
    "            elif key in not_align_drugs_entity_set:\n",
    "                for value_k,value_v in value.items():\n",
    "                    if value_k+\".dict\" not in modify_corresponding_to_files_and_entity:\n",
    "                        modify_corresponding_to_files_and_entity[value_k+\".dict\"] ={key:\"Delete\"}\n",
    "                        continue\n",
    "                    else:\n",
    "                        if key not in modify_corresponding_to_files_and_entity[value_k+\".dict\"]:\n",
    "                            modify_corresponding_to_files_and_entity[value_k + \".dict\"][key] = \"Delete\"\n",
    "                            continue\n",
    "                        else:\n",
    "                            continue\n",
    "\n",
    "            elif key in manual_vertification_align_entity_dict:\n",
    "                align_c = manual_vertification_align_entity_dict[key] # align category\n",
    "                for value_k,value_v in value.items():\n",
    "                    if value_k+\".dict\" not in modify_corresponding_to_files_and_entity:\n",
    "                        modify_corresponding_to_files_and_entity[value_k+\".dict\"] ={key:align_c}\n",
    "                        continue\n",
    "                    else:\n",
    "                        if key not in modify_corresponding_to_files_and_entity[value_k+\".dict\"]:\n",
    "                            modify_corresponding_to_files_and_entity[value_k + \".dict\"][key] = align_c\n",
    "                            continue\n",
    "                        else:\n",
    "                            continue\n",
    "                            \n",
    "    print(\"finish obtain !!!\")\n",
    "\n",
    "    print(\"start deal with align relation dicts...\")\n",
    "    for key_rel_dict_file, value_entity_category in modify_corresponding_to_files_and_entity.items():\n",
    "        print(\"start deal with {} file\".format(key_rel_dict_file))\n",
    "        read_rel_dict_path = os.path.join(read_entity_relation_dict_dir,key_rel_dict_file)\n",
    "        rel_dict_lines = readLinesFile(read_rel_dict_path)\n",
    "        os.remove(read_rel_dict_path) #delete the file\n",
    "        c1,c2 = key_rel_dict_file.split(\"_\")[:-1]\n",
    "        for line in rel_dict_lines:\n",
    "            line =json.loads(line)\n",
    "            e1 = line[\"e1\"]\n",
    "            e2 = line[\"e2\"]\n",
    "\n",
    "            if e1 not in value_entity_category and e2 not in value_entity_category:\n",
    "                writeJsonFile(line,os.path.join(write_old_rel_dict_dir,key_rel_dict_file)) #read_rel_dict_path == write_rel_dict_path\n",
    "                continue\n",
    "            #todo: The entity that need to align is unique and exisit!! so, which can obtain category firstly!!!\n",
    "            # todo: note , have entity not exesit!\n",
    "            # align_c1 = value_entity_category[e1]\n",
    "            # align_c2 = value_entity_category[e2]\n",
    "            # if align_c1 == \"Delete\"  or align_c2 == \"Delete\":\n",
    "            #     continue\n",
    "            elif (e1 in value_entity_category and c1 != value_entity_category[e1]) and (e2 in value_entity_category and c2 != value_entity_category[e2]): # two entity need to align\n",
    "                align_c1 = value_entity_category[e1]\n",
    "                align_c2 = value_entity_category[e2]\n",
    "                if align_c1 == \"Delete\" or align_c2 == \"Delete\":\n",
    "                    continue\n",
    "                if align_c1+\"_\"+align_c2 in all_relation_map:\n",
    "                    writeJsonFile(line, os.path.join(write_old_rel_dict_dir,all_relation_map[align_c1+\"_\"+align_c2]))  # read_rel_dict_path == write_rel_dict_path\n",
    "            elif (e1 in value_entity_category and c1 != value_entity_category[e1]) and e2 not in value_entity_category:\n",
    "                align_c1 = value_entity_category[e1]\n",
    "                if align_c1 == \"Delete\":\n",
    "                    continue\n",
    "                if align_c1 +\"_\"+ c2 in all_relation_map:\n",
    "                    writeJsonFile(line, os.path.join(write_old_rel_dict_dir,all_relation_map[align_c1+\"_\"+ c2]))\n",
    "            elif (e2 in value_entity_category and c2 != value_entity_category[e2]) and e1 not in value_entity_category:\n",
    "                align_c2 = value_entity_category[e2]\n",
    "                if align_c2 == \"Delete\":\n",
    "                    continue\n",
    "                if c1+\"_\"+align_c2 in all_relation_map:\n",
    "                    writeJsonFile(line, os.path.join(write_old_rel_dict_dir,all_relation_map[c1+\"_\"+align_c2]))\n",
    "        print(\"finish  {} file deal with\".format(key_rel_dict_file))\n",
    "    end_time = time.localtime(time.time())\n",
    "    print(\"end time \",end_time)\n",
    "\n",
    "\n",
    "def deleteLeq2EntpairOfRelationDict(read_entity_relation_dict_dir,write_new_rel_dict_dir):\n",
    "    print(\"start delete less than equal 2 entpair...\")\n",
    "    rel_dict_file_list = os.listdir(read_entity_relation_dict_dir)\n",
    "    stat_leg_2_entpair_amount = 0\n",
    "    for rel_dict_file_name in rel_dict_file_list:\n",
    "        if rel_dict_file_name == \".DS_Store\":\n",
    "            continue\n",
    "        print(\"start delete {} less than equal 2 entpair...\".format(rel_dict_file_name))\n",
    "        rel_dict_path = os.path.join(read_entity_relation_dict_dir, rel_dict_file_name)\n",
    "        lines = readLinesFile(rel_dict_path)\n",
    "        for line in lines:\n",
    "            line = json.loads(line)\n",
    "            if len(line[\"e1\"]) <=2 or len(line[\"e2\"]) <=2:\n",
    "                stat_leg_2_entpair_amount += 1\n",
    "                continue\n",
    "            else:\n",
    "                writeJsonFile(line, os.path.join(write_new_rel_dict_dir,rel_dict_file_name))\n",
    "\n",
    "        print(\"finish delete {} less than equal 2 entpair\".format(rel_dict_file_name))\n",
    "    print(\"delete <=2 entpairs = {}\".format(stat_leg_2_entpair_amount))\n",
    "if __name__ == \"__main__\":\n",
    "    #todo: note: execute below function, the order is very important!!!\n",
    "    #todo: statistics not align entity!!!, this is the first statistics!\n",
    "    print(\"Start run time :\",time.localtime(time.time()))\n",
    "    read_entity_relation_dict_dir = \"./Data/quchong_relation_dict_v2.0/\"\n",
    "    write_stat_path =\"./Data/rel_dicts_same_entity_category_different_1_1.json\"\n",
    "#    statRelDictsSameEntityDifferentCategoryAmount(read_entity_relation_dict_dir, write_stat_path)\n",
    "\n",
    "    #todo: the first align relation dict\n",
    "    read_all_relation_map_path = \"./Data/all_relation_map.json\"\n",
    "    read_stat_diff_category_file_path = \"./Data/rel_dicts_same_entity_category_different_1_1.json\"\n",
    "    write_old_rel_dict_dir = \"./Data/quchong_relation_dict_v2.0/\" # the same dir deal with!!\n",
    "#    relationDictFilesAlign(read_all_relation_map_path, read_stat_diff_category_file_path,read_entity_relation_dict_dir, write_old_rel_dict_dir)\n",
    "\n",
    "    #todo: statistics again, this is the second statistics!\n",
    "    write_stat_2_path = \"./Data/rel_dicts_same_entity_category_different_2.json\"\n",
    "#    statRelDictsSameEntityDifferentCategoryAmount(read_entity_relation_dict_dir, write_stat_2_path)\n",
    "\n",
    "\n",
    "    # belong to drugs align drugsbank, others statisics result, which need manual verification!!\n",
    "    read_drugbank_file_path = \"./Data/drugbank\"\n",
    "    read_stat_not_align_file_path = \"./Data/rel_dicts_same_entity_category_different_2.json\"\n",
    "    write_stat_dir = \"./Data/stat_align_drugbank_dir_2/\"\n",
    "#    judgeDrugsWhetherInDrugBank(read_drugbank_file_path, read_stat_not_align_file_path, write_stat_dir)\n",
    "\n",
    "    # statistics for lijun ,csv format\n",
    "    read_stat_dir = write_stat_dir\n",
    "    write_stat_csv_format_dir = \"./Data/stat_align_drugbank_csv_format_dir/\"\n",
    "#    statFormatCSV(read_stat_dir, write_stat_csv_format_dir)\n",
    "\n",
    "\n",
    "    #todo: the second, deal with align drugs, not align entity and manual vertification by lijun\n",
    "    read_stat_2_diff_category_file_path =\"./Data/rel_dicts_same_entity_category_different_2.json\"\n",
    "    read_align_drugs_entity_path = \"./Data/stat_align_drugbank_dir_2/align_drug.json\"\n",
    "    read_not_align_drugs_entity_path = \"./Data/stat_align_drugbank_dir_2/drugs_not_align_drugsbank.json\"\n",
    "    read_manual_vertify_align_entity_path = \"./Data/stat_align_drugbank_dir_2/manual_verification.csv\"\n",
    "#    relationDictFilesAlignImprove(read_all_relation_map_path, read_stat_2_diff_category_file_path,read_align_drugs_entity_path, read_not_align_drugs_entity_path,read_manual_vertify_align_entity_path, read_entity_relation_dict_dir,write_old_rel_dict_dir)\n",
    "\n",
    "    # delete less than 2 entpairs !!\n",
    "    write_new_rel_dict_dir = \"./Data/NewEntityRelationDict_v2.1/\" # match the version of entity relation dict!!!\n",
    "#    deleteLeq2EntpairOfRelationDict(read_entity_relation_dict_dir, write_new_rel_dict_dir)\n",
    "\n",
    "\n",
    "    #todo: statistics again, this is the third statistics!\n",
    "    read_entity_relation_dict_3_dir = \"./Data/NewEntityRelationDict_v2.1/\"\n",
    "    write_stat_3_path = \"./Data/rel_dicts_same_entity_category_different_4.json\"\n",
    "    statRelDictsSameEntityDifferentCategoryAmount(read_entity_relation_dict_dir=read_entity_relation_dict_3_dir, write_stat_path = write_stat_3_path)\n",
    "    print(\"End run time :\",time.localtime(time.time()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y\n"
     ]
    }
   ],
   "source": [
    "a = \"asd\"\n",
    "if a in \"asdfgh\":\n",
    "    print(\"y\")\n",
    "elif a in \"asdfghjk\":\n",
    "    print(\"n\")\n",
    "else:\n",
    "    print(\"yulongsha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

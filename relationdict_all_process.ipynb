{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:compound_protein_treat.dict\n",
      "0\n",
      "100000\n",
      "2:gene_organism_belong.dict\n",
      "0\n",
      "3:equipment_disease_diagnose|treat.dict\n",
      "0\n",
      "4:drugs_anatomy_affect.dict\n",
      "0\n",
      "5:compound_disease_cause.dict\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "6:gene_disease_cause.dict\n",
      "0\n",
      "100000\n",
      "7:gene_protein_express.dict\n",
      "0\n",
      "8:drugs_organism_apply.dict\n",
      "9:disease_anatomy_occur.dict\n",
      "0\n",
      "10:compound_drugs_contain.dict\n",
      "0\n",
      "11:protein_anatomy_belong.dict\n",
      "0\n",
      "12:gene_gene_interact.dict\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "13:gene_anatomy_locate.dict\n",
      "0\n",
      "100000\n",
      "14:drugs_disease_treat.dict\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "15:disease_disease_coexist.dict\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "16:drugs_protein_target.dict\n",
      "0\n",
      "100000\n",
      "17:protein_protein_interact.dict\n",
      "0\n",
      "18:protein_disease_cause.dict\n",
      "0\n",
      "19:protein_organism_belong.dict\n",
      "0\n",
      "20:drugs_gene_target.dict\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "21:drugs_drugs_interact.dict\n",
      "0\n",
      "22:compound_gene_treat.dict\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import json, os, _pickle as pickle, sys\n",
    "#  读取实体字典，获得其中的 med_synonyms_name_dict字典，用于实体归一\n",
    "def loads_pick(map):\n",
    "\n",
    "#    if os.path.exists(map):\n",
    "    fs = open(map, 'rb')\n",
    "    med_synonyms_name_dict, med_name_cate_dict = pickle.loads(fs.read())  # 使用loads反序列化\n",
    "#    med_name_synonyms_dict = {v:k for k,v in med_synonyms_name_dict.items()}\n",
    "#    else:\n",
    "#        print ('not exist dict.pkl')\n",
    "#        exit(0)\n",
    "    return med_synonyms_name_dict\n",
    "\n",
    "#  将两个库中的数据合在一个文件里  （例：将 malacards 和 drugbank 的数据和在一个文件里，单独运行）\n",
    "def file_add(file1,file2):\n",
    "    with open(file1,mode='r') as file,open(file2, 'a') as fl:\n",
    "        \n",
    "        file.seek(0,0)\n",
    "        line = file.readline()\n",
    "        \n",
    "        while(line):\n",
    "            line_dict = json.loads(line.strip())        \n",
    "            fl.write(json.dumps(line_dict) + '\\n')\n",
    "            \n",
    "            line = file.readline()\n",
    "        fl.close()\n",
    "\n",
    "#  将关系字典中的实体归一，并将关系库的名称均用数字来代替    relation_dict_before_guiyi_v2.0   relation_dict_after_guiyi_v2.0\n",
    "def e1_e2_guiyi(file_name, med_synonyms_name_dict):\n",
    "    with open('./all_relation_dict_file_v2.0/relation_dict_before_guiyi_v2.0/'+file_name) as fr , open('./all_relation_dict_file_v2.0/relation_dict_after_guiyi_v2.0/'+file_name,'a') as fw:\n",
    "        check_d=[]\n",
    "        num_dict = {\"ttd\": \"-1\", \"kegg\": \"-2\", \"dgidb\": \"-3\", \"drugbank\": \"-4\", \"brenda\": \"-5\", \\\n",
    "                    \"uniprot\": \"-6\", \"chembl\": \"-7\", \"drugcentral\": \"-8\", \"malacards\": \"-9\", \"ctd\": \"-10\", \\\n",
    "                    \"biogrid\": \"-11\", \"ebi\": \"-12\", \"omim\": \"-13\", \"disgenet\": \"-14\", \"gene_organizer\": \"-15\", \\\n",
    "                    \"-1\":\"-1\", \"-2\":\"-2\", \"-3\":\"-3\", \"-4\":\"-4\", \"-5\":\"-5\", \"-6\":\"-6\", \"-7\":\"-7\", \"-8\":\"-8\", \\\n",
    "                    \"-9\":\"-9\", \"-10\":\"-10\", \"-11\":\"-11\",\"-12\":\"-12\", \"-13\":\"-13\", \"-14\":\"-14\", \"-15\":\"-15\"}\n",
    "#        for n,i in enumerate(fr.readlines()):\n",
    "        for line in fr:\n",
    "            line_dict = json.loads(line.strip())\n",
    "            flag1 = False; flag2 = False            \n",
    "#            if (n==0):\n",
    "#                print (sys.getsizeof(fr))\n",
    "#            try:\n",
    "#            json_text = json.loads(i.strip())\n",
    "            e1 = line_dict['e1'].lower()\n",
    "            e2 = line_dict['e2'].lower()\n",
    "            source = line_dict[\"sources\"]\n",
    "            line_dict[\"sources\"] = list(set([num_dict[v] for v in source]))\n",
    "            if (e1 in med_synonyms_name_dict):   #关系字典中的实体是实体字典synonyms_name_dict中的key,则将实体修改为name\n",
    "                line_dict[\"e1\"] = med_synonyms_name_dict[e1]\n",
    "                flag1 = True                    \n",
    "\n",
    "            if (e2 in med_synonyms_name_dict):   #关系字典中的实体是实体字典synonyms_name_dict中的key,则将实体修改为name\n",
    "                line_dict[\"e2\"] = med_synonyms_name_dict[e2]\n",
    "                flag2 = True\n",
    "\n",
    "            if flag1 and flag2 :\n",
    "                check_d.append([e1,e2])\n",
    "            fw.write(json.dumps(line_dict) + '\\n')\n",
    "#                yield json_text\n",
    "                ##去重\n",
    "                #fw.write(json.dumps(json_text))\n",
    "                #fw.write('\\n')\n",
    "#            except ValueError:\n",
    "                #logging.info('json format error')\n",
    "#                print ('json format errot®')\n",
    "        print('len(check_id):',len(check_d))\n",
    "\n",
    "#  将归一之后关系字典中实体既属于 e1 又属于 e2 的关系删除，同时删除实体长度小于等于2的关系对    relation_dict_after_guiyi   relation_dict_self_align1\n",
    "def entity_belong_e1_and_e2(file_name):   #  注意：像 gene_gene 这样的文件就不需要处理了\n",
    "    print(\"delete_entity_belong_e1_and_e2:\")\n",
    "    with open('./all_relation_dict_file_v2.0/relation_dict_after_guiyi_v2.0/' + file_name,'r') as file, open('./all_relation_dict_file_v2.0/relation_dict_self_align1_v2.0/' + file_name, 'w') as fl:\n",
    "        e1_set = set()\n",
    "        e2_set = set()\n",
    "        del_data = 0\n",
    "        del_data_list = []\n",
    "        file.seek(0,0)\n",
    "        for line in file:\n",
    "            line_dict = json.loads(line.strip())\n",
    "            e1 = line_dict[\"e1\"].lower()\n",
    "            e2 = line_dict[\"e2\"].lower()\n",
    "            e1_set.add(e1)\n",
    "            e2_set.add(e2)\n",
    "\n",
    "        file.seek(0,0)\n",
    "        for line in file:\n",
    "            line_dict = json.loads(line.strip())\n",
    "            e1 = line_dict[\"e1\"].lower()\n",
    "            e2 = line_dict[\"e2\"].lower()\n",
    "            if (e1 in e2_set) or (e2 in e1_set) or (len(e1) <= 2) or (len(e2) <= 2):\n",
    "                del_data += 1\n",
    "#                del_data_list.append(e1 + \"##\" + e2)\n",
    "#                print(e1 + \"##\" + e2)\n",
    "                continue\n",
    "            else:\n",
    "#                print(line_dict)\n",
    "                fl.write(json.dumps(line_dict) + '\\n')\n",
    "        print(file_name + \":\" + str(del_data))\n",
    "        \n",
    "#  合并 e1、e2 相同的实体              relation_dict_self_align1    combine_relation_dict\n",
    "def combine_relation(file_name):\n",
    "\n",
    "#for rfile in rfile_list:\n",
    "#    with open('./all_relation_dict_file_v2.0/relation_dict_self_align1_v2.0/' + file_name,mode='r') as file,open('./all_relation_dict_file_v2.0/combine_relation_dict_v2.0/' + file_name,'w+') as fl:   # 关系字典与实体字典对齐之前用这个路径\n",
    "    with open('./relation_entity_dict_align_v3.1/' + file_name,mode='r') as file,open('./all_relation_dict_file_v2.0/combine_relation_dict_v2.1/' + file_name,'w+') as fl:  # 关系字典与实体字典对齐之后用这个路径\n",
    "        file.seek(0,0)\n",
    "        line = file.readline()\n",
    "        res = {}\n",
    "        while(line):\n",
    "            line_dict = json.loads(line.strip())\n",
    "            e1 = line_dict[\"e1\"].lower()\n",
    "            e2 = line_dict[\"e2\"].lower()\n",
    "            score = line_dict[\"scores\"]\n",
    "    #            if score ==[]:\n",
    "    #                score = [\"\"]\n",
    "\n",
    "            pid = line_dict[\"pids\"]\n",
    "    #            if pid ==[]:\n",
    "    #                pid = [\"\"]                \n",
    "            source = line_dict[\"sources\"]\n",
    "\n",
    "            name_combine = e1 + '####' + e2\n",
    "            if name_combine not in res:\n",
    "                res[name_combine] = {\"e1\":e1,\"e2\":e2,\"scores\":score,\"pids\":pid,\"sources\":source}\n",
    "            else:\n",
    "                res[name_combine][\"scores\"].extend(score)\n",
    "                res[name_combine][\"pids\"].extend(pid)\n",
    "                res[name_combine][\"sources\"].extend(source)\n",
    "\n",
    "            line = file.readline()\n",
    "\n",
    "        for value in res.values():\n",
    "            fl.write(json.dumps(value) + '\\n')\n",
    "        fl.close()\n",
    "\n",
    "#  将关系中的每个字段都做去重处理                combine_relation_dict    quchong_relation_dict\n",
    "def quchong_each_field(file_name):            #  将所有文件中的数据去重\n",
    "    with open('./all_relation_dict_file_v2.0/combine_relation_dict_v2.1/'+file_name, 'r') as file, open('./all_relation_dict_file_v2.0/quchong_relation_dict_v2.1/'+file_name, 'w') as fl:\n",
    "        file.seek(0,0)\n",
    "        res = {}\n",
    "#        dict_qu = {}\n",
    "        list_qu = []\n",
    "        for i,line in enumerate(file):\n",
    "            if i % 100000 ==0:print(i)\n",
    "            line_dict = json.loads(line.strip())\n",
    "#            res = {\"e1\":line_dict[\"e1\"],\"e2\":line_dict[\"e2\"],\"scores\":list(set(line_dict[\"scores\"])),\"pids\":list(set(line_dict[\"pids\"])),\"sources\":list(set([num_dict[v] for v in line_dict[\"sources\"]]))}\n",
    "            res = {\"e1\":line_dict[\"e1\"],\"e2\":line_dict[\"e2\"],\"scores\":list(set(line_dict[\"scores\"])),\"pids\":list(set(line_dict[\"pids\"])),\"sources\":list(set(line_dict[\"sources\"]))}\n",
    "\n",
    "#            dict_qu[json.dumps(res)] = 1\n",
    "            list_qu.append(json.dumps(res))\n",
    "        list_quchong = list(set(list_qu))\n",
    "        for key in list_quchong:\n",
    "            fl.write(key + \"\\n\")             \n",
    "        fl.close()\n",
    "#  todo:解决关系字典的自对齐问题（包含：单个关系字典的自对齐，关系字典之间的对齐）  relation_dict_self_align2\n",
    "#  todo:解决关系字典与实体字典的歧义问题(代码在本机的文件中)\n",
    "#  todo:解决完歧义问题，仅需要跑 “合并 e1,e2”;“去重” 这两个部分代码～\n",
    "\n",
    "if __name__=='__main__':\n",
    "#    file_add('./genes_diseases_cause_ctd.dict', \\\n",
    "#             './gene_disease_cause_old.dict')\n",
    "\n",
    "    map = './medical_entity_dict_v4.4.pkl'\n",
    "    med_synonyms_name_dict = loads_pick(map)\n",
    "#    rfile_list = ['compound_protein_treat.dict','disease_compound_cause.dict','disease_disease_coexist.dict', \\\n",
    "#                  'drugs_disease_treat.dict','drugs_gene_target.dict','drugs_protein_target.dict','gene_anatomy_locate.dict', \\\n",
    "#                  'gene_compound_treat.dict','gene_gene_interact.dict','gene_protein_express.dict', \\\n",
    "#                  'protein_disease_cause.dict','protein_organism_belong.dict','protein_protein_interact.dict']\n",
    "#    rfile_list = ['gene_disease_cause.dict']    #  单独跑：'gene_disease_cause.dict',\n",
    "\n",
    "#    file_folder_path = r\"./all_relation_dict_file_v2.0/relation_dict_before_guiyi_v2.0\"  #  第一遍跑用该路径\n",
    "    file_folder_path = r\"./relation_entity_dict_align_v3.1\"       #  第二编跑用该路径\n",
    "    \n",
    "    file_path_list = []\n",
    "    for file_path_root,_,file_name_list in os.walk(file_folder_path):\n",
    "        for file_name in file_name_list:\n",
    "#            if (\"compound_disease_cause.dict\" in file_name) or (\"compound_gene_treat.dict\" in file_name) \\\n",
    "#                or (\"compound_protein_treat.dict\" in file_name):\n",
    "            if \".dict\" in file_name:\n",
    "                file_path_list.append(file_name)\n",
    "    num = 0\n",
    "    for file_name in file_path_list:\n",
    "        num = num + 1\n",
    "        print(str(num) + \":\" + file_name)\n",
    "        e1_e2_guiyi(file_name,med_synonyms_name_dict)  #  将关系字典中的实体归一，并将关系库的名称均用数字来代替 \n",
    "        entity_belong_e1_and_e2(file_name)           #  将归一之后关系字典中实体既属于 e1 又属于 e2 的关系删除\n",
    "        combine_relation(file_name)                #  合并 e1、e2 相同的实体\n",
    "        quchong_each_field(file_name)            #  将关系中的每个字段都做去重处理\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compare compound_protein_treat.dict file\n",
      "finish compound_protein_treat.dict file\n",
      "compare gene_organism_belong.dict file\n",
      "finish gene_organism_belong.dict file\n",
      "compare equipment_disease_diagnose|treat.dict file\n",
      "finish equipment_disease_diagnose|treat.dict file\n",
      "compare drugs_anatomy_affect.dict file\n",
      "finish drugs_anatomy_affect.dict file\n",
      "compare compound_disease_cause.dict file\n",
      "finish compound_disease_cause.dict file\n",
      "compare gene_disease_cause.dict file\n",
      "finish gene_disease_cause.dict file\n",
      "compare gene_protein_express.dict file\n",
      "finish gene_protein_express.dict file\n",
      "compare drugs_organism_apply.dict file\n",
      "finish drugs_organism_apply.dict file\n",
      "compare disease_anatomy_occur.dict file\n",
      "finish disease_anatomy_occur.dict file\n",
      "compare compound_drugs_contain.dict file\n",
      "finish compound_drugs_contain.dict file\n",
      "compare protein_anatomy_belong.dict file\n",
      "finish protein_anatomy_belong.dict file\n",
      "compare gene_gene_interact.dict file\n",
      "finish gene_gene_interact.dict file\n",
      "compare gene_anatomy_locate.dict file\n",
      "finish gene_anatomy_locate.dict file\n",
      "compare drugs_disease_treat.dict file\n",
      "finish drugs_disease_treat.dict file\n",
      "compare disease_disease_coexist.dict file\n",
      "finish disease_disease_coexist.dict file\n",
      "compare drugs_protein_target.dict file\n",
      "finish drugs_protein_target.dict file\n",
      "compare protein_protein_interact.dict file\n",
      "finish protein_protein_interact.dict file\n",
      "compare protein_disease_cause.dict file\n",
      "finish protein_disease_cause.dict file\n",
      "compare protein_organism_belong.dict file\n",
      "finish protein_organism_belong.dict file\n",
      "compare drugs_gene_target.dict file\n",
      "finish drugs_gene_target.dict file\n",
      "compare drugs_drugs_interact.dict file\n",
      "finish drugs_drugs_interact.dict file\n",
      "compare compound_gene_treat.dict file\n",
      "finish compound_gene_treat.dict file\n",
      "实体关系不在实体字典与黑名单中的实体个数 5189\n"
     ]
    }
   ],
   "source": [
    "#  统计关系字典与实体字典的所有实体歧义类型\n",
    "import os\n",
    "import json\n",
    "from nltk import pos_tag\n",
    "from collections import Counter\n",
    "\n",
    "def writeJsonFile(dictLineJson, write_json_file_path):\n",
    "    with open(write_json_file_path, \"a\",encoding=\"utf-8\") as fp:\n",
    "        json.dump(dictLineJson, fp)  # write file\n",
    "        fp.write(\"\\n\")\n",
    "\n",
    "def readJsonFile(read_json_file_path):\n",
    "    with open(read_json_file_path, \"r\",encoding=\"utf-8\") as fp:\n",
    "        json_dict = json.load(fp)\n",
    "    return json_dict\n",
    "\n",
    "def readPickleData(read_pkl_path):\n",
    "    with open(read_pkl_path,\"rb\") as fp:\n",
    "        pkl_data = pickle.load(fp)\n",
    "    return pkl_data\n",
    "\n",
    "def readLineFile(read_file_path):\n",
    "    with open(read_file_path,\"r\",encoding=\"utf-8\") as fp:\n",
    "        lines = fp.readlines()\n",
    "    return lines\n",
    "\n",
    "def writeFile(str , write_file_path):\n",
    "    with open(write_file_path,\"a\",encoding=\"utf-8\") as fp:\n",
    "        fp.write(str + \"\\n\")\n",
    "\n",
    "# statistics between  Relation Dict and Name_Category Dict different entity ratio!!!\n",
    "def statERandEDictCategoryDifferentRatio(black_entity_path, entity_relation_dict_dir, name_category_entity_path,\n",
    "                                         write_entity_relation_dict_dir, write_unexist_entity_path):\n",
    "    name_category_dict = readJsonFile(name_category_entity_path)\n",
    "    entity_relation_dict_list = os.listdir(entity_relation_dict_dir)\n",
    "    statistics_different_entity_category = 0\n",
    "#    different_entity_category_dict = {}\n",
    "    unexist_entity_set = set()\n",
    "#    category_statis_dict = {\"compound\": {}, \"disease\": {}, \"drugs\": {}, \"gene\": {}, \"organism\": {}, \"protein\": {}}\n",
    "    black_entity_set = set()\n",
    "    black_dict_lines = readLineFile(black_entity_path)\n",
    "    for black_entity in black_dict_lines:\n",
    "        black_entity_set.add(black_entity.strip())\n",
    "\n",
    "    for entity_relation_file_name in entity_relation_dict_list:\n",
    "        if entity_relation_file_name == \".DS_Store\" or entity_relation_file_name ==\".ipynb_checkpoints\":\n",
    "            continue\n",
    "        print(\"compare {} file\".format(entity_relation_file_name))\n",
    "        entity_relation_dict_path = entity_relation_dict_dir + entity_relation_file_name\n",
    "        entity_relation_dict_lines = readLineFile(entity_relation_dict_path)\n",
    "        c1 = entity_relation_file_name.split(\"_\")[0]\n",
    "        c2 = entity_relation_file_name.split(\"_\")[1]\n",
    "        for line in entity_relation_dict_lines:\n",
    "            entity_category_dict = {} # temp save different category of entity, which relation dict and name_category dict!\n",
    "            #print(\"line =\",line)\n",
    "            line = json.loads(line)\n",
    "            e1 = line[\"e1\"]\n",
    "            e2 = line[\"e2\"]\n",
    "            source = line[\"sources\"]  # list [\"-2\",\"-4\"]\n",
    "            if e1 == \"\" or e2 == \"\":\n",
    "                continue\n",
    "            e1_flag = e1 in name_category_dict\n",
    "            e2_flag = e2 in name_category_dict\n",
    "            if e1_flag and c1 != name_category_dict[e1]:\n",
    "                statistics_different_entity_category += 1\n",
    "                # category_statis_dict[c1][e1] = name_category_dict[e1]\n",
    "                entity_category_dict[e1] = c1 + \"_\" + name_category_dict[e1]\n",
    "                entity_category_dict[\"source\"] = source\n",
    "                # different_entity_category_dict[e1] = c1 + \"_\"+name_category_dict[e1]\n",
    "            if not e1_flag and e1 not in black_entity_set:\n",
    "                if not (pos_tag([e1])[0][-1] in [\"NN\", \"NNS\", \"NNP\", \"NNPS\"] or (\n",
    "                        pos_tag([e1])[0][-1] in [\"JJ\"] and (',' in e1 or '-' in e1))):\n",
    "                    unexist_entity_set.add((e1, c1, str(source)))\n",
    "            if e2_flag and c2 != name_category_dict[e2]:\n",
    "                statistics_different_entity_category += 1\n",
    "                # category_statis_dict[c2][e2] = name_category_dict[e2]\n",
    "                # different_entity_category_dict[e1] = c2 + \"_\" + name_category_dict[e2]\n",
    "                entity_category_dict[e2] = c2 + \"_\" + name_category_dict[e2]\n",
    "                entity_category_dict[\"source\"] = source\n",
    "            if not e2_flag and e2 not in black_entity_set:\n",
    "                if not (pos_tag([e2])[0][-1] in [\"NN\", \"NNS\", \"NNP\", \"NNPS\"] or (\n",
    "                        pos_tag([e2])[0][-1] in [\"JJ\"] and (',' in e2 or '-' in e2))):\n",
    "                    unexist_entity_set.add((e2, c2, str(source)))\n",
    "            if entity_category_dict != {}:\n",
    "                writeJsonFile(entity_category_dict, write_entity_relation_dict_dir + entity_relation_file_name)\n",
    "        print(\"finish {} file\".format(entity_relation_file_name))\n",
    "\n",
    "    print(\"实体关系不在实体字典与黑名单中的实体个数\", len(unexist_entity_set))\n",
    "\n",
    "    writeJsonFile(list(unexist_entity_set), write_unexist_entity_path)#todo: 存储json格式的不能存储set数据，而set数据中不能存储list数据！！！\n",
    "\n",
    "\n",
    "def statisDifferCategory(category_statis_dict_path):\n",
    "    category_statis_dict = readJsonFile(category_statis_dict_path)\n",
    "    counter = Counter()\n",
    "    for key, value in category_statis_dict.items():\n",
    "        value_value = value.values()\n",
    "        category_stat = Counter(value_value)\n",
    "        print(\"实体关系中属于{}而实体字典中不属于它的实体个数{} \".format(key, len(value)), category_stat)\n",
    "\n",
    "def trimGiveLiJunData(read_trim_data_path, write_trim_data_path):\n",
    "    trim_dict = readJsonFile(read_trim_data_path)\n",
    "    # trim_dict = dict(trim_dict)\n",
    "    trim_value_dict = trim_dict[\"protein\"]\n",
    "    line_flag = 0\n",
    "    for key, value in trim_value_dict.items():\n",
    "        if line_flag == 0:\n",
    "            title = \"entity\" + \"  \" + \"relation_dict_category\" + \"    \" + \"entity_dict_category\"\n",
    "            writeFile(title, write_trim_data_path)\n",
    "        data_line = key + \"   \" + \"protein\" + \"   \" + value\n",
    "        writeFile(data_line, write_trim_data_path)\n",
    "        line_flag += 1\n",
    "\n",
    "dict_num = {'-1': 'ttd', '-2': 'kegg', '-3': 'dgidb', '-4': 'drugbank', '-5': 'brenda', '-6': 'uniprot', '-7': 'chembl', '-8': 'drugcentral', '-9': 'malacards', '-10': 'ctd', '-11': 'biogrid', '-12': 'ebi', '-13': 'omim', '-14': 'disgenet', '-15': 'gene_organizer'}\n",
    "\n",
    "def combineCategoryDifferentDict(read_combine_dict_dir,write_combine_finish_dir,write_combine_chihui_dir):\n",
    "    combine_file_name_list = os.listdir(read_combine_dict_dir)\n",
    "    for combine_file_name in combine_file_name_list:\n",
    "        if combine_file_name == \".ipynb_checkpoints\":\n",
    "            continue\n",
    "        combine_file_path = read_combine_dict_dir + combine_file_name\n",
    "        lines = readLineFile(combine_file_path)\n",
    "        combine_different_category_dict = {}\n",
    "        c1 = combine_file_name[:-5].split(\"_\")[0]\n",
    "        c2 = combine_file_name[:-5].split(\"_\")[1]\n",
    "        record_summary_dict_1 ={c1:\"\"}\n",
    "        record_summary_dict_2 = {c2: \"\"}\n",
    "        record_summary_dict_3 = {c1 + \"_\" + c2 : \"\"}\n",
    "        count_1 = Counter()\n",
    "        count_2 = Counter()\n",
    "        count_3 = Counter()\n",
    "        for line in lines:\n",
    "            line = json.loads(line)\n",
    "            c3_1 = \"\"\n",
    "            c3_2 = \"\"\n",
    "            for key, value in line.items():\n",
    "                if key != \"source\":\n",
    "                    combine_different_category_dict[key] = value\n",
    "                    value_c1 = value.split(\"_\")[0]\n",
    "                    value_c1_1 = value.split(\"_\")[1]\n",
    "                    #一行中e1实体不相同的\n",
    "                    if value_c1 == c1:\n",
    "                        source_list = sorted(line[\"source\"])\n",
    "                        for source_id in source_list:\n",
    "                            count_1[value_c1_1 +\"(\"+ dict_num[source_id]+\")\"] += 1\n",
    "                    #一行中e2实体不相同的\n",
    "                    if value_c1 == c2:\n",
    "                        source_list = sorted(line[\"source\"])\n",
    "                        for source_id in source_list:\n",
    "                            count_2[value_c1_1 +\"(\"+ dict_num[source_id]+\")\"] += 1\n",
    "                    #一行两个实体的类别都不相同都\n",
    "                    if len(line) == 3:\n",
    "\n",
    "                        if value_c1 == c1:\n",
    "                            c3_1 = value_c1_1\n",
    "                            continue\n",
    "                        if value_c1 == c2:\n",
    "                            c3_2 = value_c1_1\n",
    "                        source_list = sorted(line[\"source\"])\n",
    "                        for source_id in source_list:\n",
    "                            count_3[c3_1+\"_\"+c3_2 + \"(\" + dict_num[source_id]+\")\"] += 1\n",
    "\n",
    "        record_summary_dict_1[c1]=count_1\n",
    "        record_summary_dict_2[c2] = count_2\n",
    "        record_summary_dict_3[c1 + \"_\" + c2] = count_3\n",
    "        write_combine_finish_file_path = write_combine_finish_dir + combine_file_name\n",
    "        writeJsonFile(record_summary_dict_1,write_combine_finish_file_path)\n",
    "        writeJsonFile(record_summary_dict_2, write_combine_finish_file_path)\n",
    "        writeJsonFile(record_summary_dict_3,write_combine_finish_file_path)\n",
    "\n",
    "\n",
    "        #给迟慧将文件中不同的类别总结在一个字典中\n",
    "        write_combine_chihui_file_path = os.path.join(write_combine_chihui_dir , combine_file_name)\n",
    "        writeJsonFile(combine_different_category_dict, write_combine_chihui_file_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #todo: statistics the information, which entity category of entity and name_category dict different!!!\n",
    "    black_entity_path = \"./Data/black_dict_v3.0.txt\"\n",
    "#    entity_relation_dict_dir = \"./Data/NewEntityRelationDict_v2.1/\"\n",
    "#    entity_relation_dict_dir = \"./relation_entity_dict_align_v3.0/\"\n",
    "    entity_relation_dict_dir = \"./all_relation_dict_file_v2.0/quchong_relation_dict_v2.1/\"\n",
    "    name_category_entity_path = \"./Data/medical_entity_Name_Category_v4.4.json\"\n",
    "    write_entity_relation_dict_dir = \"./Data/EntityRelationDictInfo_v3.3/\" # save information of  differnet entity category!\n",
    "    write_unexist_entity_path = \"./Data/unexist_entity_v3.3.json\"\n",
    "    statERandEDictCategoryDifferentRatio(black_entity_path, entity_relation_dict_dir, name_category_entity_path,\n",
    "                                         write_entity_relation_dict_dir, write_unexist_entity_path)\n",
    "\n",
    "    #todo: statistics in detail!!!\n",
    "    # read_trim_data_path = \"./Data/category_statis_dcit.json\"\n",
    "    # write_trim_data_path  = \"./SatisticsData/不正常的实体类别统计.txt\"\n",
    "    # trimGiveLiJunData(read_trim_data_path, write_trim_data_path)\n",
    "\n",
    "    #todo: combine different entity category for chihui\n",
    "    read_combine_dict_dir = \"./Data/EntityRelationDictInfo_v3.3/\"\n",
    "    write_combine_finish_dir = \"./Data/SummaryLiJunDifferentCategoryDict_v3.3/\"\n",
    "    write_combine_chihui_dir = \"./Data/SummaryChihuiDifferentCategoryDict_v3.3/\"\n",
    "    combineCategoryDifferentDict(read_combine_dict_dir, write_combine_finish_dir, write_combine_chihui_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
